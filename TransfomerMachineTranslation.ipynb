{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Data <a name=\"2-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = pd.read_csv('./small_vocab_en.csv', header=None, usecols=[0])\n",
    "fr_df = pd.read_csv('./small_vocab_fr.csv', header=None, usecols=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(en_df), len(fr_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences =  en_df[0].values\n",
    "french_sentences = fr_df[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(english_sentences)):\n",
    "    english_sentences[i] = \"sos \" + str(english_sentences[i]) + \" eos.\"\n",
    "    french_sentences[i] = \"sos \" + str(french_sentences[i]) + \" eos.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization <a name=\"2-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "tokenizer_en = Tokenizer(num_words=num_words, filters='!#$%&()*+,-/:;<=>@«»\"\"[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer_en.fit_on_texts(english_sentences)\n",
    "english_sentences = tokenizer_en.texts_to_sequences(english_sentences)\n",
    "\n",
    "word_index = tokenizer_en.word_index\n",
    "print(f\"The number of words in the English vocabulary: {len(word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fr = Tokenizer(num_words=num_words, filters='!#$%&()*+,-/:;<=>@«»\"\"[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer_fr.fit_on_texts(french_sentences)\n",
    "french_sentences = tokenizer_fr.texts_to_sequences(french_sentences)\n",
    "\n",
    "word_index_fr = tokenizer_fr.word_index\n",
    "print(f\"The number of words in the French vocabulary: {len(word_index_fr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding <a name=\"2-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = pad_sequences(english_sentences, maxlen = 30, padding='post', truncating='post')\n",
    "french_sentences = pad_sequences(french_sentences, maxlen=30, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, embedding_dim):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(embedding_dim))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, embedding_dim):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n",
    "                           np.arange(embedding_dim)[np.newaxis, :], embedding_dim)\n",
    "    \n",
    "    # apply sin to even indices in the array. ie 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # apply cos to odd indices in the array. ie 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking   <a name=\"3-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    \n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inputs, targets):\n",
    "\n",
    "    enc_padding_mask = create_padding_mask(inputs)\n",
    "\n",
    "    dec_padding_mask = create_padding_mask(inputs)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
    "\n",
    "    dec_target_padding_mask = create_padding_mask(targets)\n",
    "    \n",
    "\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], dtype=tf.float32)\n",
    "    scaled_dk = tf.math.sqrt(dk)\n",
    "    \n",
    "\n",
    "    scaled_attention_logits = matmul_qk / scaled_dk\n",
    "    \n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, key_dim, num_heads, dropout_rate=0.0):\n",
    "\n",
    "        super(MyMultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "\n",
    "        assert key_dim % num_heads == 0 \n",
    "        self.depth = self.key_dim // self.num_heads\n",
    "        \n",
    "        self.wq = Dense(key_dim)\n",
    "        self.wk = Dense(key_dim)\n",
    "        self.wv = Dense(key_dim)\n",
    "    \n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "\n",
    "        self.dense = Dense(key_dim)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask=None):\n",
    "\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n",
    "        output = self.dense(concat_attention)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected NeuralNetwork <a name=\"4-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(embedding_dim, fully_connected_dim):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),\n",
    "        tf.keras.layers.Dense(embedding_dim)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1):\n",
    "\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "\n",
    "        self.mha = MyMultiHeadAttention(embedding_dim, num_heads, dropout_rate)\n",
    "        \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "        self.ffn = FeedForward(embedding_dim, fully_connected_dim)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        \n",
    "        out1 = self.layernorm1(attn_output + x)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout(ffn_output, training=training)\n",
    "        \n",
    "        out2 = self.layernorm2(ffn_output + out1)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = Embedding(input_vocab_size, embedding_dim)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, embedding_dim)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training, mask):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "        inputs = self.embedding(inputs)\n",
    "\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "\n",
    "        inputs += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        inputs = self.dropout(inputs, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i](inputs, training, mask)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1):\n",
    "\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "\n",
    "        self.mha1 = MyMultiHeadAttention(embedding_dim, num_heads, dropout_rate)\n",
    "        self.mha2 = MyMultiHeadAttention(embedding_dim, num_heads, dropout_rate)\n",
    "        \n",
    "\n",
    "        self.ffn = FeedForward(embedding_dim, fully_connected_dim)\n",
    "        \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        \n",
    "        out1 = self.layernorm1(attn1 + x) \n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        \n",
    "\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        \n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = Embedding(target_vocab_size, embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, embedding_dim)\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "\n",
    "            attention_weights[f\"decoder_layer{i + 1}_block1\"] = block1\n",
    "            attention_weights[f\"decoder_layer{i + 1}_block2\"] = block2\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, target_vocab_size, max_positional_encoding_input, max_positional_encoding_target, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, max_positional_encoding_input, dropout_rate)\n",
    "        self.decoder = Decoder(num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, max_positional_encoding_target, dropout_rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "        \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        \n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Hyperparameters <a name=\"5-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 256  \n",
    "fully_connected_dim = 512 \n",
    "num_layers = 4 \n",
    "num_heads = 8 \n",
    "dropout_rate = 0.1 \n",
    "\n",
    "\n",
    "input_vocab_size = len(tokenizer_fr.word_index) + 2  \n",
    "target_vocab_size = len(tokenizer_en.word_index) + 2  \n",
    "\n",
    "\n",
    "max_positional_encoding_input = input_vocab_size  \n",
    "max_positional_encoding_target = target_vocab_size  \n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, embedding_dim, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.embedding_dim = tf.cast(embedding_dim, dtype=tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embedding_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer = Transformer(num_layers, embedding_dim, num_heads,\n",
    "                           fully_connected_dim, input_vocab_size, target_vocab_size, \n",
    "                           max_positional_encoding_input, max_positional_encoding_target, dropout_rate)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2 = 0.98, epsilon = 1e-9)\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "def loss_function(true_values, predictions):\n",
    "\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(true_values, 0))\n",
    "\n",
    "   \n",
    "    loss_ = loss_object(true_values, predictions)\n",
    "\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    \n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(true_values, predictions):\n",
    "\n",
    "    accuracies = tf.equal(true_values, tf.argmax(predictions, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(true_values, 0))\n",
    "\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(batch_size, 30), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(batch_size,30), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(encoder_input, target):\n",
    "\n",
    "    decoder_input = target[:, :-1]\n",
    "\n",
    "    expected_output = target[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(encoder_input, decoder_input, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "\n",
    "        loss = loss_function(expected_output, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(expected_output, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    current_batch_index = 0\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(int(len(english_sentences)/batch_size)):\n",
    "        target_batch = tf.convert_to_tensor(np.array(english_sentences[current_batch_index:current_batch_index+batch_size]),dtype=tf.int64)\n",
    "        input_batch = tf.convert_to_tensor(np.array(french_sentences[current_batch_index:current_batch_index+batch_size]),dtype=tf.int64)\n",
    "\n",
    "        current_batch_index = current_batch_index + batch_size\n",
    "        train_step(input_batch, target_batch)\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "def translate_helper(sentence):\n",
    "    sentence = 'sos ' + sentence[0] + ' eos.'\n",
    "    sentence = [sentence] \n",
    "    \n",
    "\n",
    "    sentence = tokenizer_fr.texts_to_sequences(sentence)\n",
    "    sentence = pad_sequences(sentence, maxlen=30, padding='post', truncating='post')\n",
    "    input = tf.convert_to_tensor(np.array(sentence),dtype=tf.int64)\n",
    "    \n",
    "\n",
    "    decoder_input = tokenizer_en.texts_to_sequences(['sos'])\n",
    "    decoder_input = tf.convert_to_tensor(np.array(decoder_input), dtype=tf.int64)\n",
    "    \n",
    "\n",
    "    for i in range(maxlen):\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input, decoder_input)\n",
    "        \n",
    "        predictions, _ = transformer(input, decoder_input,False,enc_padding_mask,combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predictions = predictions[: ,-1:, :] \n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int64)\n",
    "        \n",
    "        if predicted_id == tokenizer_en.texts_to_sequences(['eos']):\n",
    "            return tf.squeeze(decoder_input, axis=0)\n",
    "        \n",
    "\n",
    "        decoder_input = tf.concat([decoder_input, predicted_id], axis=1)\n",
    "    \n",
    "    return tf.squeeze(decoder_input, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Function <a name=\"6-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "\n",
    "    \n",
    "    sentence = [sentence]\n",
    "    \n",
    "\n",
    "    print(f'Input sentence: {sentence[0]}')\n",
    "    print()\n",
    "    \n",
    "\n",
    "    result = (translate_helper(sentence)).tolist()\n",
    "    \n",
    "\n",
    "    predicted_ids = [i for i in result if i != tokenizer_en.texts_to_sequences(['sos'])[0][0]\n",
    "                     and i != tokenizer_en.texts_to_sequences(['eos.'])[0][0]]\n",
    "    \n",
    "    predicted_sentence = tokenizer_en.sequences_to_texts([predicted_ids])\n",
    "    \n",
    "    print(f'Translation: {predicted_sentence[0]}')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"new jersey est parfois calme pendant l' automne\"\n",
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"california est généralement calme en mars\"\n",
    "translate(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
